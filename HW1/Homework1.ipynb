{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58aa2227-c5f1-431b-bc9a-5b16a811115d",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd831cc8-9189-4a31-a09f-9d4cd8870b2f",
   "metadata": {},
   "source": [
    "Find a formula for the exact gradient of the sum of squares loss fucntion based on a linear regression model with d+1 weights.\n",
    "\n",
    "![Question 1](question_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05754155-4528-4e02-85fc-f6ecc63c0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve Imports\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841a3a4-317e-4265-9532-7b5a6f8dc8b0",
   "metadata": {},
   "source": [
    "### Question 2 & 3\n",
    "\n",
    "Write a class for Linear Regression and implement elastic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f3b2c3a-ff10-44d2-bb7f-fda5b40c99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for Linear Regression\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=0.01, l1=0, l2=0):\n",
    "        # Initialize Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "    \n",
    "    # Sum Squared Error loss\n",
    "    def sum_sq_err(self, X, Y):\n",
    "        err = self.predict(X) - Y\n",
    "        return np.sum(err**2)\n",
    "    \n",
    "    # Derivative of Sum Squared Error loss \n",
    "    def d_sum_sq_err(self, X, Y):\n",
    "        err = self.predict(X) - Y\n",
    "        X = np.hstack((np.ones((self.num_examples, 1)), X)) # add bias\n",
    "        return np.sum((2 * err).reshape(-1, 1) * X, 0)\n",
    "    \n",
    "    # Composite loss function\n",
    "    def loss(self, X, Y):\n",
    "        return self.sum_sq_err(X, Y) + self.l1 * np.sum(np.abs(self.w)) + self.l2 * self.w @ self.w\n",
    "    \n",
    "    # Derivative of composite loss function\n",
    "    def d_loss(self, X, Y):\n",
    "        return self.d_sum_sq_err(X, Y) + self.l1 * np.sign(self.w) + 2 * self.l2 * self.w\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.num_examples = X.shape[0]\n",
    "        assert self.num_examples == Y.shape[0] # Make sure every input has a pair\n",
    "        \n",
    "        self.input_dim = X.shape[1] # Register number of inputs\n",
    "        \n",
    "        self.w = np.random.rand(self.input_dim + 1) # initialize weights\n",
    "        \n",
    "        # Perform the gradient descent\n",
    "        for i in range(self.max_iterations):\n",
    "            \n",
    "            # Update gradient\n",
    "            gradient = self.d_loss(X, Y)\n",
    "            \n",
    "            # Step down the gradient\n",
    "            self.w -= self.learning_rate * gradient\n",
    "            \n",
    "            # Display loss\n",
    "            if i % int(self.max_iterations / 10) == 0:\n",
    "                print(\"Loss (\" + str(100.0 * i / self.max_iterations), \"%)\", self.loss(X, Y))\n",
    "            \n",
    "            # Break early in the case of convergence\n",
    "            if np.linalg.norm(gradient) < self.tolerance:\n",
    "                print(\"Converged in\", i, \"iterations.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to converge after\", self.max_iterations, \"iterations.\")\n",
    "            \n",
    "        # Print results\n",
    "        print(\"Training Loss\", self.loss(X, Y))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X)) # add bias\n",
    "        Y_hat = X @ self.w\n",
    "        \n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4270f6-0afb-476a-bd55-0560c7372217",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Use the diabetes dataset to test your implementation which 10 different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43e1539d-a03c-4154-a38a-6ae3ac9ca7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load the diabetes dataset\n",
    "\n",
    "dataset = datasets.load_diabetes(return_X_y=False, as_frame=False)\n",
    "X = dataset['data']\n",
    "Y = dataset['target']\n",
    "\n",
    "# Preprocess data a little\n",
    "\n",
    "# Split training set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size=0.4, random_state=1)\n",
    "# Split validation sets\n",
    "(devX, testX, devY, testY) = train_test_split(testX, testY, test_size=0.5, random_state=1)\n",
    "\n",
    "# Normalize data (using only information from the training set)\n",
    "x_scale = np.max(np.abs(trainX), 0)\n",
    "y_scale = np.max(np.abs(trainY), 0)\n",
    "\n",
    "trainX /= x_scale\n",
    "trainY /= y_scale\n",
    "\n",
    "devX /= x_scale\n",
    "devY /= y_scale\n",
    "\n",
    "testX /= x_scale\n",
    "testY /= y_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2ed62-9f8c-45f1-ac0e-55d0284b7e09",
   "metadata": {},
   "source": [
    "#### Begin Iterating on Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4278abc2-7bbc-439e-bb07-30713aef8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 273.65322756182604\n",
      "Loss (10.0 %) 4.364580136470097e+127\n",
      "Loss (20.0 %) 5.396301942861854e+254\n",
      "Loss (30.0 %) inf\n",
      "Loss (40.0 %) inf\n",
      "Loss (50.0 %) nan\n",
      "Loss (60.0 %) nan\n",
      "Loss (70.0 %) nan\n",
      "Loss (80.0 %) nan\n",
      "Loss (90.0 %) nan\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss nan\n",
      "Testing Loss  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619041/824025048.py:18: RuntimeWarning: overflow encountered in square\n",
      "  return np.sum(err**2)\n",
      "/home/kapow_12/Development/MTH4320/HW1/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/kapow_12/Development/MTH4320/HW1/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/tmp/ipykernel_1619041/824025048.py:32: RuntimeWarning: invalid value encountered in multiply\n",
      "  return self.d_sum_sq_err(X, Y) + self.l1 * np.sign(self.w) + 2 * self.l2 * self.w\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 1\n",
    "\n",
    "model = LinearRegression(0.01, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 273.65322756182604\n",
    "# Loss (10.0 %) 4.364580136470097e+127\n",
    "# Loss (20.0 %) 5.396301942861854e+254\n",
    "# Loss (30.0 %) inf\n",
    "# Loss (40.0 %) inf\n",
    "# Loss (50.0 %) nan\n",
    "# Loss (60.0 %) nan\n",
    "# Loss (70.0 %) nan\n",
    "# Loss (80.0 %) nan\n",
    "# Loss (90.0 %) nan\n",
    "# Failed to converge after 1000 iterations.\n",
    "# Training Loss nan\n",
    "# Testing Loss  nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b71f53f-5631-4e19-abe1-b5b7c8c0ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 823.5190915264616\n",
      "Loss (10.0 %) 1.6230284557585647e+45\n",
      "Loss (20.0 %) 1.682541830746321e+89\n",
      "Loss (30.0 %) 1.7442374795675064e+133\n",
      "Loss (40.0 %) 1.8081953919556403e+177\n",
      "Loss (50.0 %) 1.8744985208667138e+221\n",
      "Loss (60.0 %) 1.9432328609859092e+265\n",
      "Loss (70.0 %) inf\n",
      "Loss (80.0 %) inf\n",
      "Loss (90.0 %) inf\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss inf\n",
      "Testing Loss  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619041/824025048.py:18: RuntimeWarning: overflow encountered in square\n",
      "  return np.sum(err**2)\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 2\n",
    "\n",
    "model = LinearRegression(0.005, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 823.5190915264616\n",
    "# Loss (10.0 %) 1.6230284557585647e+45\n",
    "# Loss (20.0 %) 1.682541830746321e+89\n",
    "# Loss (30.0 %) 1.7442374795675064e+133\n",
    "# Loss (40.0 %) 1.8081953919556403e+177\n",
    "# Loss (50.0 %) 1.8744985208667138e+221\n",
    "# Loss (60.0 %) 1.9432328609859092e+265\n",
    "# Loss (70.0 %) inf\n",
    "# Loss (80.0 %) inf\n",
    "# Loss (90.0 %) inf\n",
    "# Failed to converge after 1000 iterations.\n",
    "# Training Loss inf\n",
    "# Testing Loss  inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9660c6-bcc2-4a14-9c55-91931d5f75f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 58.08458560579729\n",
      "Loss (10.0 %) 6.525142531557556\n",
      "Loss (20.0 %) 6.475068760019262\n",
      "Loss (30.0 %) 6.467266360164314\n",
      "Loss (40.0 %) 6.465797801056767\n",
      "Loss (50.0 %) 6.465346269620829\n",
      "Loss (60.0 %) 6.46510631779223\n",
      "Loss (70.0 %) 6.464943058124732\n",
      "Loss (80.0 %) 6.464824668152927\n",
      "Loss (90.0 %) 6.46473764827031\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss 6.464674060711005\n",
      "Testing Loss  2.4148525792443607\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 3\n",
    "\n",
    "model = LinearRegression(0.0025, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 58.08458560579729\n",
    "# Loss (10.0 %) 6.525142531557556\n",
    "# Loss (20.0 %) 6.475068760019262\n",
    "# Loss (30.0 %) 6.467266360164314\n",
    "# Loss (40.0 %) 6.465797801056767\n",
    "# Loss (50.0 %) 6.465346269620829\n",
    "# Loss (60.0 %) 6.46510631779223\n",
    "# Loss (70.0 %) 6.464943058124732\n",
    "# Loss (80.0 %) 6.464824668152927\n",
    "# Loss (90.0 %) 6.46473764827031\n",
    "# Failed to converge after 1000 iterations.\n",
    "# Training Loss 6.464674060711005\n",
    "# Testing Loss  2.4148525792443607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8eb640-293d-44bb-a55f-4e083b3752a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 60.47415289087634\n",
      "Loss (10.0 %) 6.703438564555817\n",
      "Loss (20.0 %) 6.537827204958671\n",
      "Loss (30.0 %) 6.503417215639195\n",
      "Loss (40.0 %) 6.490966939089154\n",
      "Loss (50.0 %) 6.48368948515207\n",
      "Loss (60.0 %) 6.478602794786781\n",
      "Loss (70.0 %) 6.474892512155621\n",
      "Loss (80.0 %) 6.472162115164595\n",
      "Loss (90.0 %) 6.470149228027221\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss 6.468677488016695\n",
      "Testing Loss  2.421986986787128\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 4\n",
    "\n",
    "model = LinearRegression(0.0025, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 60.47415289087634\n",
    "# Loss (10.0 %) 6.703438564555817\n",
    "# Loss (20.0 %) 6.537827204958671\n",
    "# Loss (30.0 %) 6.503417215639195\n",
    "# Loss (40.0 %) 6.490966939089154\n",
    "# Loss (50.0 %) 6.48368948515207\n",
    "# Loss (60.0 %) 6.478602794786781\n",
    "# Loss (70.0 %) 6.474892512155621\n",
    "# Loss (80.0 %) 6.472162115164595\n",
    "# Loss (90.0 %) 6.470149228027221\n",
    "# Failed to converge after 1000 iterations.\n",
    "# Training Loss 6.468677488016695\n",
    "# Testing Loss  2.421986986787128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6621b434-6622-4aac-acaf-6d3038ee6a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 40.247884991758525\n",
      "Converged in 1361 iterations.\n",
      "Training Loss 6.464574999125547\n",
      "Testing Loss  2.412140527931596\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 5\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 40.247884991758525\n",
    "# Converged in 1361 iterations.\n",
    "# Training Loss 6.464574999125547\n",
    "# Testing Loss  2.412140527931596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add3b8f2-0e01-43c6-9410-1de7975b2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 50.764264317883764\n",
      "Converged in 811 iterations.\n",
      "Training Loss 6.464575019074128\n",
      "Testing Loss  2.414304991670213\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 6\n",
    "\n",
    "model = LinearRegression(0.003, 100000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 50.764264317883764\n",
    "# Converged in 811 iterations.\n",
    "# Training Loss 6.464575019074128\n",
    "# Testing Loss  2.414304991670213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a4b697-6e9a-46f3-a578-d231b521d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 65.17460975779105\n",
      "Converged in 1097 iterations.\n",
      "Training Loss 6.559311324265084\n",
      "Testing Loss  2.486464887780107\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 7\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.01, 0, 0.2)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 65.17460975779105\n",
    "# Converged in 1097 iterations.\n",
    "# Training Loss 6.559311324265084\n",
    "# Testing Loss  2.486464887780107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a41a3e-54fa-4367-898e-1d598fd22a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 78.42300207300728\n",
      "Converged in 1605 iterations.\n",
      "Training Loss 6.514941528535085\n",
      "Testing Loss  2.450092901245724\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 8\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.01, 0, 0.1)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 78.42300207300728\n",
    "# Converged in 1605 iterations.\n",
    "# Training Loss 6.514941528535085\n",
    "# Testing Loss  2.450092901245724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87c60ff-b241-4687-8669-4e5c961dbb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 49.44017802301459\n",
      "Loss (10.0 %) 6.630548047312789\n",
      "Loss (20.0 %) 6.630549710673954\n",
      "Loss (30.0 %) 6.630553377838671\n",
      "Loss (40.0 %) 6.630544972046762\n",
      "Loss (50.0 %) 6.6305404773429215\n",
      "Loss (60.0 %) 6.630542025083202\n",
      "Loss (70.0 %) 6.630543801395358\n",
      "Loss (80.0 %) 6.630545882208487\n",
      "Loss (90.0 %) 6.630548436930064\n",
      "Failed to converge after 100000 iterations.\n",
      "Training Loss 6.630548359158723\n",
      "Testing Loss  2.5518461465079745\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 9\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.01, 0.1, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 49.44017802301459\n",
    "# Loss (10.0 %) 6.630548047312789\n",
    "# Loss (20.0 %) 6.630549710673954\n",
    "# Loss (30.0 %) 6.630553377838671\n",
    "# Loss (40.0 %) 6.630544972046762\n",
    "# Loss (50.0 %) 6.6305404773429215\n",
    "# Loss (60.0 %) 6.630542025083202\n",
    "# Loss (70.0 %) 6.630543801395358\n",
    "# Loss (80.0 %) 6.630545882208487\n",
    "# Loss (90.0 %) 6.630548436930064\n",
    "# Failed to converge after 100000 iterations.\n",
    "# Training Loss 6.630548359158723\n",
    "# Testing Loss  2.5518461465079745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312ee996-5518-404d-8536-e4eacbbdcfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 82.83463540417561\n",
      "Converged in 3657 iterations.\n",
      "Training Loss 6.464494123132788\n",
      "Testing Loss  2.4130901081116827\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 10\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.001, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 82.83463540417561\n",
    "# Converged in 3657 iterations.\n",
    "# Training Loss 6.464494123132788\n",
    "# Testing Loss  2.4130901081116827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a09fa50-e5d4-4346-9393-b9afa4877786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 27.410044287390072\n",
      "Converged in 2452 iterations.\n",
      "Training Loss 6.46449412348736\n",
      "Final Loss 2.0969771182557913\n",
      "[ 0.07796146 -0.11410416  0.18673767 -0.02716914 -0.16106449 -0.10440444\n",
      " -0.02261146  0.11850674 -0.11205969 -0.14959122 -0.08940687 -0.39669056\n",
      " -0.08502443  0.14672059  0.22073634 -0.10200458 -0.17971746 -0.04786453\n",
      "  0.12193329  0.08126257  0.04833053 -0.20906995  0.30949391 -0.24974171\n",
      "  0.02925708  0.0221328   0.13742009  0.11169439 -0.11311636 -0.02780616\n",
      " -0.19724854  0.00894599  0.08653555  0.03877369  0.21707322  0.09411205\n",
      " -0.28138069 -0.26109369  0.05273458  0.06277213 -0.12794386 -0.0631305\n",
      " -0.24185486  0.08439682  0.14816407 -0.20234096  0.06871584  0.0860322\n",
      "  0.07814577 -0.04616101  0.09407826  0.20055499  0.04670688 -0.00888967\n",
      "  0.00990211 -0.0112129  -0.38244892 -0.05752455  0.10118458 -0.19451447\n",
      " -0.02016404  0.02772898  0.04856779  0.22107448  0.05920242  0.08444856\n",
      "  0.08774493  0.01559515 -0.09175342 -0.11403511  0.31786831 -0.19395577\n",
      " -0.05067661  0.08607267 -0.18917648 -0.29972758  0.09188342 -0.02746712\n",
      "  0.14032334 -0.16302372 -0.31401173  0.13565476  0.12868879  0.34819788\n",
      "  0.04157821  0.04338892  0.07687491  0.00414458 -0.18225955]\n"
     ]
    }
   ],
   "source": [
    "# Test on Test Set\n",
    "\n",
    "model = LinearRegression(0.0025, 100000, 0.001, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Final Loss\", model.loss(testX, testY))\n",
    "\n",
    "# Loss (0.0 %) 69.72746598230765\n",
    "# Converged in 1878 iterations.\n",
    "# Training Loss 6.464494123655863\n",
    "# Final Loss 2.096977125900896"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc61346-b0cc-4fff-b5cf-bef87f19a08d",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda11ed7-f1e0-4878-81ec-78c0e68ff6a8",
   "metadata": {},
   "source": [
    "![Question 5](question_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdbd3d-d8f2-48f0-9eb5-5b50570f7c58",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Find the new exact derivative of the loss function.\n",
    "\n",
    "![Question 6](question_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a289950-fbca-400d-8a3d-f702fab4f335",
   "metadata": {},
   "source": [
    "Write an implementation for a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24be87b7-bf9d-4440-9d7d-cc62a6b869e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for Logistic Classifier\n",
    "\n",
    "class LogisticClassifier(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=0.01, l1=0, l2=0):\n",
    "        # Initialize Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "    \n",
    "    # X includes leading column of ones\n",
    "    \n",
    "    # Sum Squared Error loss\n",
    "    def sum_sq_err(self, X, Y):\n",
    "        err = self.predict(X) - Y\n",
    "        return np.sum(err**2)\n",
    "    \n",
    "    # Derivative of Sum Squared Error loss \n",
    "    def d_sum_sq_err(self, X, Y):\n",
    "        Y_hat = self.predict(X)\n",
    "        err = Y_hat - Y\n",
    "        X = np.hstack((np.ones((self.num_examples, 1)), X)) # add bias\n",
    "        return np.sum((2 * err).reshape(-1, 1) * self.d_y_hat(X), 0)\n",
    "    \n",
    "    # Composite loss function\n",
    "    def loss(self, X, Y):\n",
    "        return self.sum_sq_err(X, Y) + self.l1 * np.sum(np.abs(self.w)) + self.l2 * self.w @ self.w\n",
    "    \n",
    "    # Derivative of composite loss function\n",
    "    def d_loss(self, X, Y):\n",
    "        return self.d_sum_sq_err(X, Y) + self.l1 * np.sign(self.w) + 2 * self.l2 * self.w\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.num_examples = X.shape[0]\n",
    "        assert self.num_examples == Y.shape[0] # Make sure every input has a pair\n",
    "        \n",
    "        self.input_dim = X.shape[1] # Register number of inputs\n",
    "        \n",
    "        self.w = np.random.rand(self.input_dim + 1) # initialize weights\n",
    "        \n",
    "        # Perform the gradient descent\n",
    "        for i in range(self.max_iterations):\n",
    "            \n",
    "            # Update gradient\n",
    "            gradient = self.d_loss(X, Y)\n",
    "            \n",
    "            # Step down the gradient\n",
    "            self.w -= self.learning_rate * gradient\n",
    "            \n",
    "            # Display loss\n",
    "            if i % int(self.max_iterations / 10) == 0:\n",
    "                print(\"Loss (\" + str(100.0 * i / self.max_iterations), \"%)\", self.loss(X, Y))\n",
    "            \n",
    "            if np.linalg.norm(gradient) < self.tolerance:\n",
    "                print(\"Converged in\", i, \"iterations.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to converge after\", self.max_iterations, \"iterations.\")\n",
    "            \n",
    "        # Print results\n",
    "        print(\"Training Loss\", self.loss(X, Y))\n",
    "        \n",
    "    def y_hat(self, X):\n",
    "        \n",
    "        Y_hat = X @ self.w\n",
    "        Y_hat = 1 / (1 + np.exp(-Y_hat))\n",
    "        \n",
    "        return Y_hat\n",
    "    \n",
    "    def d_y_hat(self, X):\n",
    "        \n",
    "        Y_hat = self.y_hat(X)\n",
    "        d_Y_hat = (Y_hat * (1 - Y_hat)).reshape(-1, 1) * X\n",
    "        \n",
    "        return d_Y_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X)) # add bias\n",
    "        \n",
    "        return self.y_hat(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8e463-f015-4687-941f-c750d76f6ba0",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Use the credit card dataset to set the Binary Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81be928c-fe20-4644-b90d-fe11598f9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load the diabetes dataset\n",
    "\n",
    "dataset = np.genfromtxt(\"default of credit card clients.csv\", delimiter=\",\")\n",
    "X = dataset[1:, 1:24]\n",
    "Y = dataset[1:, 24]\n",
    "\n",
    "# Preprocess data a little\n",
    "\n",
    "# Split training set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size=0.4, random_state=1)\n",
    "# Split validation sets\n",
    "(devX, testX, devY, testY) = train_test_split(testX, testY, test_size=0.5, random_state=1)\n",
    "\n",
    "# Normalize data (using only information from the training set)\n",
    "x_shift = np.mean(trainX, 0)\n",
    "trainX -= x_shift\n",
    "x_scale = np.max(np.abs(trainX), 0)\n",
    "trainX /= x_scale\n",
    "\n",
    "devX -= x_shift\n",
    "devX /= x_scale\n",
    "\n",
    "testX -= x_shift\n",
    "testX /= x_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02a287-330d-42b8-b00e-548bbd76bc0e",
   "metadata": {},
   "source": [
    "#### Begin Iterating on Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df08fc6d-fb26-41f5-88f1-250e1c194386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3957.9999999988468\n",
      "Converged in 1 iterations.\n",
      "Training Loss 3957.9999999988468\n",
      "Testing Loss  1300.999999999614\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 1\n",
    "\n",
    "model = LogisticClassifier(0.01, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))\n",
    "\n",
    "# Loss (0.0 %) 3957.9999999988468\n",
    "# Converged in 1 iterations.\n",
    "# Training Loss 3957.9999999988468\n",
    "# Testing Loss  1300.999999999614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56c81333-60f0-4ab0-bc09-99ed29a1cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3957.9999999995407\n",
      "Converged in 1 iterations.\n",
      "Training Loss 3957.9999999995407\n",
      "Testing Loss  1300.999999999859\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 2\n",
    "\n",
    "model = LogisticClassifier(0.01, 1000, 0.001, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1762d7f4-b7da-4bcb-af4d-b61430cfc7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3957.999990478283\n",
      "Converged in 1 iterations.\n",
      "Training Loss 3957.9999904782812\n",
      "Testing Loss  1300.9999965581278\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 3\n",
    "\n",
    "model = LogisticClassifier(0.01, 1000, 0.0001, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95c4d982-f199-4e7b-a7da-1c3f6a6550f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 4175.152699086957\n",
      "Loss (10.0 %) 4055.4165766216825\n",
      "Loss (20.0 %) 4001.6916240213945\n",
      "Loss (30.0 %) 3976.4469116624864\n",
      "Loss (40.0 %) 3325.714065121097\n",
      "Loss (50.0 %) 3477.128149077549\n",
      "Loss (60.0 %) 2736.305700159391\n",
      "Loss (70.0 %) 2774.3663169193883\n",
      "Loss (80.0 %) 3020.8110553202964\n",
      "Loss (90.0 %) 2905.1236862642677\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss 4025.781565651896\n",
      "Testing Loss  1369.0093148072322\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 4\n",
    "\n",
    "model = LogisticClassifier(0.01, 1000, 0.01, 0, 0.2)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7df917d-8770-4708-8d59-a78f04cfb985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3193.168819616096\n",
      "Loss (10.0 %) 2607.385347284246\n",
      "Loss (20.0 %) 2579.9616754234094\n",
      "Loss (30.0 %) 2571.7173073807976\n",
      "Loss (40.0 %) 2568.5648838059706\n",
      "Loss (50.0 %) 2567.1250919422064\n",
      "Loss (60.0 %) 2566.34130883\n",
      "Loss (70.0 %) 2565.8365574408754\n",
      "Loss (80.0 %) 2565.4636253923377\n",
      "Loss (90.0 %) 2565.160661200774\n",
      "Failed to converge after 1000 iterations.\n",
      "Training Loss 2564.902262249078\n",
      "Testing Loss  869.0615980745172\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 5\n",
    "\n",
    "model = LogisticClassifier(0.001, 1000, 0.01, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87b0bea7-b6ac-496c-af77-4f5fae08cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3179.814215521814\n",
      "Loss (10.0 %) 2565.254739990574\n",
      "Loss (20.0 %) 2563.5312707290964\n",
      "Loss (30.0 %) 2562.5373829302234\n",
      "Loss (40.0 %) 2561.8765187116305\n",
      "Loss (50.0 %) 2561.4020441979897\n",
      "Loss (60.0 %) 2561.042262413442\n",
      "Loss (70.0 %) 2560.7578159126892\n",
      "Loss (80.0 %) 2560.5254338140394\n",
      "Loss (90.0 %) 2560.3305830059494\n",
      "Failed to converge after 10000 iterations.\n",
      "Training Loss 2560.163934530939\n",
      "Testing Loss  867.436579231429\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 6\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.1, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91524340-7a11-4faa-88ef-c5f3128cd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3214.6561689245327\n",
      "Loss (10.0 %) 2580.8131461975436\n",
      "Loss (20.0 %) 2580.7535650747923\n",
      "Loss (30.0 %) 2580.7502246512213\n",
      "Loss (40.0 %) 2580.749439223629\n",
      "Loss (50.0 %) 2580.7474942455774\n",
      "Loss (60.0 %) 2580.7475410715056\n",
      "Loss (70.0 %) 2580.747822610152\n",
      "Loss (80.0 %) 2580.747172678608\n",
      "Loss (90.0 %) 2580.7484110358337\n",
      "Failed to converge after 10000 iterations.\n",
      "Training Loss 2580.7468821657935\n",
      "Testing Loss  883.6660044075436\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 7\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.5, 1, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1592055a-d88d-4a7b-ae79-6bf4b868b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3267.4431312765923\n",
      "Converged in 919 iterations.\n",
      "Training Loss 2614.736802306295\n",
      "Testing Loss  909.1918144802141\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 8\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.5, 0, 1)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28488ef2-daeb-4039-abda-124d6793dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3332.143709069166\n",
      "Loss (10.0 %) 2614.7296809366785\n",
      "Converged in 1966 iterations.\n",
      "Training Loss 2614.683956694093\n",
      "Testing Loss  909.3006599429422\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 9\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.05, 0, 1)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59dcca64-5d50-4819-868c-66d8de7245a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3278.877478609188\n",
      "Loss (10.0 %) 2628.594603164541\n",
      "Loss (20.0 %) 2628.5916250182217\n",
      "Loss (30.0 %) 2628.5918115015593\n",
      "Loss (40.0 %) 2628.59219653009\n",
      "Loss (50.0 %) 2628.591681730324\n",
      "Loss (60.0 %) 2628.5916042166887\n",
      "Loss (70.0 %) 2628.5917629877245\n",
      "Loss (80.0 %) 2628.591816979332\n",
      "Loss (90.0 %) 2628.591619040465\n",
      "Failed to converge after 10000 iterations.\n",
      "Training Loss 2628.5915927711208\n",
      "Testing Loss  921.124098085133\n"
     ]
    }
   ],
   "source": [
    "# ITERATION 10\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.05, 1, 1)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Testing Loss \", model.loss(devX, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ebf1efe-0532-4992-b97f-d9646a37c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0.0 %) 3318.93480094106\n",
      "Loss (10.0 %) 2564.5663216927296\n",
      "Loss (20.0 %) 2563.1163113178877\n",
      "Loss (30.0 %) 2562.2632791014516\n",
      "Loss (40.0 %) 2561.6868266071124\n",
      "Loss (50.0 %) 2561.2671475064353\n",
      "Loss (60.0 %) 2560.9451592953283\n",
      "Loss (70.0 %) 2560.6880507581063\n",
      "Loss (80.0 %) 2560.476209709508\n",
      "Loss (90.0 %) 2560.2972759739982\n",
      "Failed to converge after 10000 iterations.\n",
      "Training Loss 2560.1432655229264\n",
      "Final Loss 865.9791948259233\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1733228/3099065017.py:8: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(np.round(model.predict(testX)) == trainY)\n"
     ]
    }
   ],
   "source": [
    "# Test on Test Set\n",
    "\n",
    "model = LogisticClassifier(0.001, 10000, 0.1, 0, 0)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "print(\"Final Loss\", model.loss(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3c4aa9cd-d3cb-42db-bf01-0741849cbb93",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (981410736.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1733228/981410736.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for i in range(len(testY)):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.round(model.predict(testX))\n",
    "\n",
    "print(y_hat)\n",
    "print(testY)\n",
    "\n",
    "for i in range(len(testY)):\n",
    "    if testY[i] != y_hat[i]:\n",
    "        print(test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e41ea-8d59-4258-80de-4487a0febbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTH4320",
   "language": "python",
   "name": "mth4320"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
